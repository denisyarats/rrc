# env
difficulty: 1
env: task${difficulty}
action_type: position # torque or both
action_repeat: 1
episode_length: 500
num_corners: 2
excluded_obses: desired_goal_orientation:achieved_goal_orientation:action
# teacher
use_teacher: false
teacher_model_dir: /private/home/denisy/workspace/research/tmp/rrc_simulation/scripts
teacher_model_step: 1
teacher_init_p: 0.0
teacher_max_step: 1000000
teacher_excluded_obses: desired_goal_orientation:achieved_goal_orientation
# denis curriculum
train_initializer: random
eval_initializer: random
curriculum_max_step: 1000000
curriculum_init_p: 0.0
# train
num_train_steps: 1000000
num_seed_steps: 1000
num_train_iters: 1
replay_buffer_capacity: ${num_train_steps}
seed: 1
# eval
eval_frequency: 10000
num_eval_episodes: 10
# misc
log_frequency_step: 10000
log_save_tb: true
save_video: true
video_fps: 50
device: cuda
# saving
save_frequency: 10000
# global params
lr: 1e-4
batch_size: 128
parameterization: clipped
actor_stddev: 0.2
hidden_depth: 2
hidden_dim: 1024
nstep: 5
random_nstep: true
mixer_init_value: 0.001
mixer_final_value: 10.0
mixer_start: 10000
mixer_period: 10000
alpha_0: 2.0
alpha_1: 10.0
alpha_2: 10.0
w0: 1.0
w1: 10.0
w2: 1.0
w3: 1.0


agent:
  name: ddpg
  class: ddpg.DDPGAgent
  params:
    obs_shape: ??? # to be specified later
    obs_slices: ??? # to be specified later
    action_shape: ??? # to be specified later
    action_range: ??? # to be specified later
    reward_shape: ??? # to be specified later
    device: ${device}
    critic_cfg: ${critic}
    actor_cfg: ${actor}
    reward_mixer_cfg: ${constant_reward_mixer}
    discount: 0.99
    lr: ${lr}
    actor_update_frequency: 1
    critic_tau: 0.01
    critic_target_update_frequency: 1
    batch_size: ${batch_size}
    nstep: ${nstep}
    use_ln: true
    excluded_obses: ${excluded_obses}

critic:
  class: ddpg.Critic
  params:
    obs_shape: ???
    action_shape: ${agent.params.action_shape}
    hidden_dim: ${hidden_dim}
    hidden_depth: ${hidden_depth}
    use_ln: ${agent.params.use_ln}

actor:
  class: ddpg.Actor
  params:
    obs_shape: ???
    action_shape: ${agent.params.action_shape}
    hidden_dim: ${hidden_dim}
    hidden_depth: ${hidden_depth}
    stddev: ${actor_stddev}
    parameterization: ${parameterization}
    use_ln: ${agent.params.use_ln}
    
reward_mixer:
  class: ddpg.RewardMixer
  params:
    reward_shape: ${agent.params.reward_shape}
    init_value: ${mixer_init_value}
    final_value: ${mixer_final_value}
    start: ${mixer_start}
    period: ${mixer_period}
    #starts: [${mixer_start}, ${mixer_start}]
    #periods: [${mixer_period}, ${mixer_period}]


fixed_reward_mixer:
  class: ddpg.FixedRewardMixer
  params:
    reward_shape: ${agent.params.reward_shape}
    alpha_0: ${alpha_0}
    alpha_1: ${alpha_1}
    alpha_2: ${alpha_2}
    
    
constant_reward_mixer:
  class: ddpg.ConstantRewardMixer
  params:
    reward_shape: ${agent.params.reward_shape}
    w0: ${w0}
    w1: ${w1}
    w2: ${w2}
    w3: ${w3}

experiment: bench

# hydra configuration
hydra:
  name: ${env}
  run:
    dir: ./exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${now:%H%M%S}_${agent.name}_${experiment}
    subdir: ${hydra.job.num}
  launcher:
    params:
      queue_parameters:
        slurm:
          max_num_timeout: 100000
          time: 4319
          #partition: learnfair
          partition: priority
          comment: iclr_deadline_sep_22
    mem_limit: 64
