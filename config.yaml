# env
use_old_simulator: false
difficulty: 1
env: task${difficulty}
action_type: position # torque or both
delta_pos: false # determines whether to use delta position actions
delta: 0.1
action_repeat: 1
episode_length: 500
num_corners: 0
# domain randomization
randomize: false
obj_pos_noise_std: 0.001
time_step: 0.004
time_step_range: 0.001
cube_mass: 0.94
cube_mass_range: 0.03
gravity: -9.81
gravity_range: 0.02
restitution: 0.8
restitution_range: 0.03
max_velocity: 10
max_velocity_range: 0.1
lateral_friction: 0.1
lateral_friction_range: 0.01
camera_rate_fps: 10
camera_rate_fps_range: 1.0
random_robot_position: true


excluded_obses: action:desired_goal_orientation:achieved_goal_orientation
use_pretrained: false
pretrained_model_dir: /private/home/denisy/workspace/research/rrc/pretrained_agents/phase1
# teacher
use_teacher: false
teacher_model_dir: /private/home/denisy/workspace/research/rrc/pretrained_agents/phase1 # /private/home/denisy/workspace/research/rrc/exp/2020.11.05/120800_ddpg_task14_old_rew_rnd/0/model
teacher_model_step: 1 #4200000
teacher_init_p: 1.0
teacher_max_step: 3000000
teacher_excluded_obses: action:object_position_diff
# denis curriculum
train_initializer: random
eval_initializer: random
curriculum_max_step: 1000000
curriculum_init_p: 0.0
# train
num_train_steps: 1000000
num_seed_steps: 1000
num_train_iters: 1
replay_buffer_capacity: ${num_train_steps}
seed: 1
# eval
eval_frequency: 10000 
num_eval_episodes: 10
# misc
log_frequency_step: 10000
log_save_tb: true
save_video: true
save_train_video: false
video_fps: 10
device: cuda
# saving
save_frequency: 10000
# global params
lr: 1e-4
batch_size: 128
parameterization: clipped
actor_stddev: 0.2
hidden_depth: 2
hidden_dim: 1024
nstep: 5
random_nstep: true



agent:
  name: ddpg
  class: ddpg.DDPGAgent
  params:
    obs_shape: ??? # to be specified later
    obs_slices: ??? # to be specified later
    action_shape: ??? # to be specified later
    action_range: ??? # to be specified later
    device: ${device}
    critic_cfg: ${critic}
    actor_cfg: ${actor}
    discount: 0.99
    lr: ${lr}
    actor_update_frequency: 1
    critic_tau: 0.01
    critic_target_update_frequency: 1
    batch_size: ${batch_size}
    nstep: ${nstep}
    use_ln: true
    excluded_obses: ${excluded_obses}

critic:
  class: ddpg.Critic
  params:
    obs_shape: ???
    action_shape: ${agent.params.action_shape}
    hidden_dim: ${hidden_dim}
    hidden_depth: ${hidden_depth}
    use_ln: ${agent.params.use_ln}

actor:
  class: ddpg.Actor
  params:
    obs_shape: ???
    action_shape: ${agent.params.action_shape}
    hidden_dim: ${hidden_dim}
    hidden_depth: ${hidden_depth}
    stddev: ${actor_stddev}
    parameterization: ${parameterization}
    use_ln: ${agent.params.use_ln}
    
experiment: bench

# hydra configuration
hydra:
  name: ${env}
  run:
    dir: ./exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${now:%H%M%S}_${agent.name}_${experiment}
    subdir: ${hydra.job.num}
  launcher:
    params:
      queue_parameters:
        slurm:
          max_num_timeout: 100000
          time: 4319
          #partition: learnfair
          partition: priority
          comment: iclr_deadline_sep_22
    mem_limit: 64
